local transformer_model = "bert-base-multilingual-cased";
local transformer_dim = 768;
//local transformer_model = "xlm-roberta-large";
//local transformer_dim = 1024;

local max_len = 128;
{
    "random_seed": 8446,
    "numpy_seed": 8446,
    "pytorch_seed": 8446,
    "dataset_reader": {
        "type": "machamp_universal_reader",
        "target_max_tokens": max_len,
        "source_max_tokens": max_len,
        "do_lowercase": false,

        "token_indexers": {
            "tokens": {
                "type": "pretrained_transformer_mixmatched",
                "max_length": max_len,
                "model_name": transformer_model
            }
        },
        "tokenizer": {
            "type": "pretrained_transformer",
            "add_special_tokens": false,
            "model_name": transformer_model
        },
        "target_token_indexers": {
            "tokens": {
                "namespace": "target_words"
            }
        },
        "target_tokenizer":{
             "type": "bert_basic_tokenizer"
        }
    },
    "vocabulary": {
        "max_vocab_size": {"target_words": 50000},
        "min_count": {
            "source_words": 1,
            "target_words": 1
        },
        "non_padded_namespaces": ["dataset"]
    },
    "model": {
        "type": "machamp_model",
        "dataset_embeds_dim": 0,
        "decoders": {
            "default": {
                "input_dim": transformer_dim,
                "loss_weight": 1,
                "order": 1
            },
            "dependency": {
                "type": "machamp_dependency_decoder",
                "arc_representation_dim": transformer_dim,
                "tag_representation_dim": 256,
                "use_mst_decoding_for_validation": true
            }
        },
        "default_max_sents": std.parseInt(std.extVar('MAX_SENTS')),
        "dropout": 0.3,
        "encoder": {
            "type": "cls_pooler",
            "cls_is_last_token": false,
            "embedding_dim": transformer_dim
        },
        "text_field_embedder": {
            "type": "basic",
            "token_embedders": {
                "tokens": {
                    "type": "machamp_pretrained_transformer_mismatched",
                    "last_layer_only": true,
                    "max_length": max_len,
                    "model_name": transformer_model,
                    "train_parameters": true
                }
            }
        }
    },
    "validation_data_loader": {
        "batch_sampler": {
            "sorting_keys": ["tokens"],
            "type": "dataset_buckets",
            "batch_size": std.parseInt(std.extVar('BATCH_SIZE')),
        }
    },
    "data_loader": {
        "batch_sampler": {
            "type": "acl_sampler",
            "max_tokens": 1024,
            //TODO: this should not have to be repeated in the trainer
            //"controller": "acl_controller",
            "controller": {
                "type": "acl_controller",
                "phi": std.extVar('PHI'),
            },
            "batch_size": std.parseInt(std.extVar('BATCH_SIZE')),
            "sorting_keys": ["tokens"],
        }
    },
    "trainer": {
        "type": "gradient_descent_acl",
        "controller": {
            "type": "acl_controller",
            "phi": std.extVar('PHI'),
        },
        "checkpointer": {
            "num_serialized_models_to_keep": 1
        },
        "use_amp": false, // could save some memory on gpu
        "grad_norm": 1,
        "learning_rate_scheduler": {
            "type": "slanted_triangular",
            "cut_frac": 0.2,
            "decay_factor": 0.38,
            "discriminative_fine_tuning": true,
            "gradual_unfreezing": true
        },
        "num_epochs": std.parseInt(std.extVar('EPOCHS')),
        "optimizer": {
            "type": "huggingface_adamw",
            "betas": [0.9, 0.99],
            "correct_bias": false,
            "lr": 0.00001,
            "parameter_groups": [
                [
                    [
                        "^_text_field_embedder.*"
                    ],
                    {}
                ],
                [
                    [
                        "^decoders.*",
                        "dataset_embedder.*"

                    ],
                    {}
                ]
            ],
            "weight_decay": 0.01
        },
        //"patience": 5, // disabled, because slanted_triangular changes the lr dynamically
        "validation_metric": "+.run/.sum"
    },
    "datasets_for_vocab_creation": [
        "train",
        "validation"//TODO can this be removed now that we add padding/unknown?
    ]
}
